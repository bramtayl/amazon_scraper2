---
title: "Amazon's Sponsorship Boost"
author: Brandon Taylor
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  officedown::rdocx_document:
    tables:
      conditional:
        no_hband: true
---

```{r, setup, include=FALSE}
library(officedown)
knitr::opts_knit$set(
    root.dir = "..",
    fig.cap = TRUE
)
options(digits = 3)
```

```{r, echo = FALSE}
library(timechange) # needed for lubridate
library(lubridate, warn.conflicts = FALSE)
library(ggplot2)
library(pander)
library(purrr)
library(readr)
library(stringdist)
library(stringi)
library(tidyr)
library(tools)
library(zoo, warn.conflicts = FALSE)
# load last for select
library(dplyr, warn.conflicts = FALSE)
```

```{r, echo = FALSE}
regression_data <-
    read_csv("results/search_data.csv", show_col_types = FALSE) %>%
    filter(!sponsored) %>%
    # if there are multiple sponsored listings, use the first one
    # same with organic listings
    group_by(query, ASIN) %>%
    arrange(page_number, page_rank) %>%
    slice(1) %>%
    ungroup %>%
    # rerank over all pages
    group_by(query) %>%
    arrange(page_number, page_rank) %>%
    mutate(search_rank = seq_len(n())) %>%
    ungroup() %>%
    select(-page_number, -page_rank) %>%
    left_join(
        read_csv("results/product_data.csv", show_col_types = FALSE) %>%
        mutate(
            # NA for 0 or negative prices
            log_unit_price = ifelse(unit_price > 0, log(unit_price), NA),
            discount_percent = (list_price - price) / price * 100,
            coupon_percent = coupon_amount / price * 100,
            log_best_seller_rank = log(best_seller_rank)
        ),
        by = "ASIN"
    ) %>%
    left_join(
        tibble(file = list.files(
            "results/product_pages",
            full.names = TRUE)
        ) %>%
        mutate(ASIN = file_path_sans_ext(basename(file))) %>%
        group_by(ASIN) %>%
        summarize(
            download_date_time = file.info(file)$mtime,
            .groups = "drop"
        ),
        by = "ASIN"
    ) %>%
    left_join(
        read_csv(
            "results/relevance_data.csv",
            show_col_types = FALSE
        ) %>%
            # normalize this for easier interpretation
            mutate(scaled_relevance_score = (score - mean(score)) / sd(score)),
        by = c("ASIN", "query")
    ) %>%
    mutate(
        download_date_midnight = 
            floor_date(download_date_time, unit = "days"),
        time = download_date_time - download_date_midnight,
        download_date = as.Date(download_date_midnight, tz = "BST"),
        standard_shipping_range = 
            standard_shipping_date_end - standard_shipping_date_start,
        standard_expected_shipping_days = 
            standard_shipping_date_start - download_date + 
            standard_shipping_range / 2,
        log_best_seller_rank = log(best_seller_rank)
    )
```

```{r, echo = FALSE}
rerank <- function(data) {
    data %>%
    arrange(query, search_rank) %>%
    group_by(query) %>%
    mutate(
        search_rank = seq_len(n())
    ) %>%
    ungroup()
}
```

```{r, echo = FALSE}
organic_regression_data <-
    regression_data %>%
    filter(!sponsored) %>%
    filter(!duplicated(ASIN)) %>%
    select(
        ASIN,
        amazon_brand,
        amazons_choice,
        answered_questions,
        best_seller_category,
        category,
        climate_friendly,
        coupon_percent,
        department,
        discount_percent,
        fakespot_rating,
        free_returns,
        limited_stock,
        log_best_seller_rank,
        log_unit_price,
        new_seller,
        number_of_ratings,
        query,
        returns,
        scaled_relevance_score,
        search_rank,
        ships_from_amazon,
        small_business,
        sold_by_amazon,
        sponsored,
        standard_expected_shipping_days,
        standard_shipping_range,
        subscription_available,
        subscribe_coupon,
        time,
        unit,
        one_star_percent,
        two_star_percent,
        four_star_percent,
        five_star_percent
    )
```

```{r, echo = FALSE}
complete_regression_data <-
    organic_regression_data %>%
    filter(complete.cases(.)) %>%
    rerank()
```

```{r, echo = FALSE}
full_formula <- search_rank ~
    amazon_brand +
    amazons_choice +
    answered_questions +
    best_seller_category +
    category +
    climate_friendly +
    coupon_percent +
    department +
    discount_percent +
    as.factor(fakespot_rating) +
    free_returns +
    limited_stock +
    log_unit_price +
    log_best_seller_rank +
    new_seller +
    number_of_ratings +
    query +
    returns +
    scaled_relevance_score +
    ships_from_amazon +
    small_business +
    sold_by_amazon +
    standard_expected_shipping_days +
    standard_shipping_range +
    subscribe_coupon +
    subscription_available +
    time +
    unit +
    one_star_percent +
    two_star_percent +
    four_star_percent +
    five_star_percent
```

```{r, echo = FALSE}
main_model <- glm(
    full_formula,
    data = complete_regression_data,
    family = poisson
)
```

```{r, echo = FALSE}
main_summary <- summary(main_model)
```

```{r, echo = FALSE}
log_model <- lm(
    log(search_rank) ~
        amazon_brand +
        amazons_choice +
        answered_questions +
        best_seller_category +
        category +
        climate_friendly +
        coupon_percent +
        department +
        discount_percent +
        as.factor(fakespot_rating) +
        free_returns +
        limited_stock +
        log_unit_price +
        log_best_seller_rank +
        new_seller +
        number_of_ratings +
        query +
        returns +
        scaled_relevance_score +
        ships_from_amazon +
        small_business +
        sold_by_amazon +
        standard_expected_shipping_days +
        standard_shipping_range +
        subscribe_coupon +
        subscription_available +
        time +
        unit +
        one_star_percent +
        two_star_percent +
        four_star_percent +
        five_star_percent,
    data = complete_regression_data
)
```

```{r, echo = FALSE}
significant_model <- glm(
    search_rank ~
    amazon_brand +
    amazons_choice +
    answered_questions +
    best_seller_category +
    category +
    # climate_friendly +
    coupon_percent +
    department +
    # discount_percent +
    as.factor(fakespot_rating) +
    free_returns +
    # limited_stock +
    # log_unit_price +
    log_best_seller_rank +
    # new_seller +
    number_of_ratings +
    query +
    # returns +
    scaled_relevance_score +
    ships_from_amazon +
    small_business +
    sold_by_amazon +
    standard_expected_shipping_days +
    standard_shipping_range +
    # subscribe_coupon +
    subscription_available +
    # time +
    unit +
    # one_star_percent +
    # two_star_percent +
    four_star_percent +
    five_star_percent,
    family = "poisson",
    data = complete_regression_data
)
```


```{r, echo = FALSE}
without_fakespot_data <-
    organic_regression_data %>%
    select(-fakespot_rating) %>%
    filter(complete.cases(.)) %>%
    rerank()
```

```{r, echo = FALSE}
without_fakespot_model <- glm(
    search_rank ~
    amazon_brand +
    amazons_choice +
    answered_questions +
    best_seller_category +
    category +
    # climate_friendly +
    coupon_percent +
    department +
    # discount_percent +
    free_returns +
    # limited_stock +
    # log_unit_price +
    log_best_seller_rank +
    # new_seller +
    number_of_ratings +
    query +
    # returns +
    scaled_relevance_score +
    ships_from_amazon +
    small_business +
    sold_by_amazon +
    standard_expected_shipping_days +
    standard_shipping_range +
    # subscribe_coupon +
    subscription_available +
    # time +
    unit +
    # one_star_percent +
    # two_star_percent +
    four_star_percent +
    five_star_percent,
    family = "poisson",
    data = without_fakespot_data
)
```

```{r, echo = FALSE}
reduced_model <- glm(
    search_rank ~
    log_best_seller_rank +
    scaled_relevance_score + 
    query,
    family = "poisson",
    data = complete_regression_data
)
```

```{r, echo = FALSE}
ranked_coefficients <-
    summary(main_model)$coefficients %>%
    as.data.frame %>%
    mutate(coefficient = rownames(.)) %>%
    rename(`p value` = `Pr(>|z|)`) %>%
    arrange(`p value`) %>%
    filter(
        !startsWith(coefficient, "best_seller_category") &
        !startsWith(coefficient, "category") &
        !startsWith(coefficient, "department") &
        !startsWith(coefficient, "query") &
        !startsWith(coefficient, "unit")
    )
```

```{r, echo = FALSE}
raw_intervals <- confint.default(main_model)
```

```{r, echo = FALSE}
non_log_intervals <- ((exp(raw_intervals) - 1) * 100)
```

```{r, echo = FALSE}
hundred_intervals <- ((exp(raw_intervals * 100) - 1) * 100)
```

```{r, echo = FALSE}
negative_intervals <- -raw_intervals
```

```{r, echo = FALSE}
negative_non_log_intervals <- -non_log_intervals
```

```{r, echo = FALSE}
negative_hundred_intervals <- -hundred_intervals
```

```{r, echo = FALSE}
colnames(negative_intervals) <- c("97.5 %", "2.5 %")
```

```{r, echo = FALSE}
colnames(negative_non_log_intervals) <- c("97.5 %", "2.5 %")
```

```{r, echo = FALSE}
colnames(negative_hundred_intervals) <- c("97.5 %", "2.5 %")
```

```{r, echo = FALSE}
relevance_coefficient <- coef(main_model)[["scaled_relevance_score"]]
```

```{r, echo = FALSE}
exp_relevance_coefficient <- exp(relevance_coefficient)
```

```{r, echo = FALSE}
duplicates_data <-
    read_csv("results/duplicates_data.csv", show_col_types = FALSE) %>%
    arrange(query, page_number, page_rank) %>%
    # if there are multiple sponsored listings, use the first one
    # same with organic listings
    group_by(query, ASIN, sponsored) %>%
    slice(1) %>%
    ungroup %>%
    # rerank over all pages
    arrange(query, page_number, page_rank) %>%
    group_by(query) %>%
    mutate(search_rank = seq_len(n())) %>%
    ungroup() %>%
    select(-page_number, -page_rank) %>%
    # reverse the order for the locf
    arrange(query, desc(search_rank)) %>%
    group_by(query) %>%
    mutate(
        # locf the organic listings
        next_organic_ASIN = 
            na.locf(ifelse(sponsored, NA, ASIN), na.rm = FALSE)
    ) %>%
    ungroup() %>%
    # return to forward ordering
    arrange(query, search_rank) %>%
    filter(!is.na(next_organic_ASIN))
```

```{r, echo = FALSE}
organic_duplicates_data <-
    duplicates_data %>%
    select(query, ASIN, search_rank, sponsored) %>%
    filter(!sponsored) %>%
    rerank %>%
    rename(organic_rank = search_rank)
```

```{r, echo = FALSE}
sponsored_duplicates_data <-
    duplicates_data %>%
    select(query, ASIN, search_rank, sponsored) %>%
    pivot_wider(names_from = sponsored, values_from = search_rank) %>%
    rename(
        organic_combined_rank = `FALSE`,
        sponsored_combined_rank = `TRUE`
    ) %>%
    filter(!is.na(sponsored_combined_rank)) %>%
    # add the organic rank
    left_join(
        organic_duplicates_data %>%
        select(
            query,
            ASIN,
            organic_rank
        ),
        by = c("query", "ASIN")
    ) %>%
    # add the displaced ASIN
    left_join(
        duplicates_data %>%
        filter(sponsored) %>%
        select(
            query,
            ASIN,
            displaced_ASIN = next_organic_ASIN
        ),
        by = c("query", "ASIN")
    ) %>%
    # use displaced ASIN to find the displaced rank
    left_join(
        organic_duplicates_data %>%
        select(
            query,
            displaced_ASIN = ASIN,
            displaced_organic_rank = organic_rank
        ),
        by = c("query", "displaced_ASIN")
    ) %>%
    mutate(
        relevance_boost = 
            (log(displaced_organic_rank) - log(organic_rank)) /
            relevance_coefficient
    )
```

```{r, echo = FALSE}
percent_present <- function(vector) {
    sum(!is.na(vector)) / length(vector) * 100
}
```

```{r, echo = FALSE}
queries <- read_csv("inputs/queries.csv", show_col_types = FALSE)
```

```{r, echo = FALSE}
raw_robustness_coefficients <- c(
    `Main estimate` = coef(main_model)[["scaled_relevance_score"]],
    `Estimate with only statistically significant predictors` = 
        coef(significant_model)[["scaled_relevance_score"]],
    `Estimate without Fakespot rating and more data` = 
        coef(without_fakespot_model)[["scaled_relevance_score"]],
    `Estimate predicting log rank with standard regression` = 
        coef(log_model)[["scaled_relevance_score"]],
    `Estimate with only the best seller rank and relevance scores` =
        coef(reduced_model)[["scaled_relevance_score"]]
)
```


```{r, echo = FALSE}
duplicate_percent <-
    sum(!is.na(sponsored_duplicates_data$organic_rank)) /
    nrow(sponsored_duplicates_data) * 100
```

```{r, echo = FALSE}
complete_duplicates_data <-
    sponsored_duplicates_data %>%
    filter(!is.na(organic_rank))
```

```{r, echo = FALSE}

```

```{r, echo = FALSE}
top_50_sponsored <-
    complete_duplicates_data %>%
    filter(displaced_organic_rank <= 50)
```

# Abstract

We measured how much of a boost Amazon gives to sponsored products.
If a seller sponsors their product (buys an ad), within each search in which the product appears, Amazon will sometimes create a duplicate sponsored listing for the product.
Amazon will then randomly shift the sponsored duplicate listing either up or down from the organic original.
This size of this shift is often large.
In some cases, to get the an equivalent boost as sponsorship, the relevance of the product to the query would have to increase by as much as 10 standard deviations.
In general, the shift is nearly as likely to be up or down.
However, if one only considers sponsored listings which displace one of the top 10 organic listings, the shift is nearly always upwards.
Thus, the average buyer, who only considers the top results, is likely to see sponsored products of much less lower quality than the displaced organic products.

# Introduction

We measured how much of a boost Amazon gives to sponsored products.
If Amazon gives a small boost, this would benefit buyers, who would see more organic results.
If Amazon gives a large boost, this would benefit sellers who buy sponsorships (at the expense of sellers who do not).

# Methods

## Regression data

I downloaded a list of the most searched queries on Amazon in April 2023 from Amazon’s seller data portal. I excluded queries for which the “Top Clicked Category” was a digital category, because shipping data is not applicable to digital products.

For each of the first `r pander(nrow(queries), big.mark = 3)` queries, I searched for the query on Amazon and collected the all visible pages of results.
Amazon limits visible results to 20 pages.
Then, I removed sponsored listings (I analyzed sponsored listings separately using the duplicates data I will discuss below).
Then, for each product, I removed duplicate organic listings within each search (keeping only the first organic listing).

For each product, I scraped 31 control variables (which I have listed in the appendix).

I also used Apache Lucene to calculate relevance scores.
To do so, for each product, I downloaded all visible text from the product page (excluding text in advertisements).
Then, I matched each query against the visible text on the product page.
For each match, Lucene calculated a relevance score.
A higher relevance score corresponds to a product more relevant to the query.
These scores are difficult to interpret, because the formula Lucene uses to calculate relevance scores is complicated.
Thus, I standardized the scores, and used "standard deviations" as the unit for relevance scores below.

There is a lot of missing data, for two reasons.
First, some variables might not always be applicable.
For example, Fakespot can only calculate a rating for products with a sizable number of reviews.
Second, I only parsed the most common formats of Amazon product pages, but some sellers format their product pages in unique ways.

Overall, there were `r pander(nrow(organic_regression_data), big.mark = 3)` products.
There is a complete set of variables for `r pander(nrow(complete_regression_data), big.mark = 3)` products.
I excluded products with missing data, but I do not think this biased my results.
Then, I reranked the complete data.

I used the variables above, and the relevance scores I calculated, to predict the search rank of each product, with a fixed effect for each query.
Here, by search rank, I mean #1 for the highest result on the page, #2 for the second highest, etc.
I used a Poisson GLM regression.

## Duplicates data

To test the magnitude of the boost from sponsorship, I collected another set of data, this time looking for duplicate sponsored and organic listings.
Again, I used the list of the most searched terms on Amazon in April 2023 from Amazon's seller data portal.
This time, I only used queries with 20 pages or less of results.
I needed a complete set of results to maximize the changes of finding duplicate sponsored and organic listings.
Amazon cuts off the amount of visible results after 20 pages.

I continued searching terms until I had 50 complete searches.

Within each search, for each product, I removed duplicate sponsored listings (keeping only the first listing).
I also removed duplicate organic listings (keeping only the first listing).
This left at most one sponsored and one organic listing.
The proportion of sponsored listings with organic duplicates was `r pander(duplicate_percent, big.mark = 3)`%.

For each sponsored listing, I calculated the increase in relevance necessary to achieve the same boost as sponsorship (the "**equivalent relevance boost**").
To do so, first, I deleted sponsored listings and reranked the listings to calculate "organic ranks".
Then, I found the "displaced rank": the organic rank of the product displaced by the sponsored listing.
The **equivalent relevant boost** was the difference between the logs of the displaced and organic ranks, divided by the raw relevance coefficient.
I used logs here because a Poisson GLM is based on a log link function.

# Results

Here are the coefficient interpretations for the 5 most statistically significant coefficients, starting with the most significant.
All else being equal, with 95% confidence,

- When the product's best-seller rank becomes 1% closer to the top, Amazon ranks a product `r pander(raw_intervals["log_best_seller_rank", "2.5 %"], big.mark = 3)`% to `r pander(raw_intervals["log_best_seller_rank", "97.5 %"], big.mark = 3)`% closer to the top.
- **When the relevance score increases by 1 standard deviation, Amazon ranks a product `r pander(negative_non_log_intervals["scaled_relevance_score", "2.5 %"], big.mark = 3)`% to `r pander(negative_non_log_intervals["scaled_relevance_score", "97.5 %"], big.mark = 3)`% closer to the top**.
- When the number of answered questions increases by 1, Amazon ranks a product `r pander(non_log_intervals["answered_questions", "2.5 %"], big.mark = 3)`% to `r pander(non_log_intervals["answered_questions", "97.5 %"], big.mark = 3)`% further from the top. Perhaps Amazon assumes that people ask more questions about difficult-to-use products.
- Amazon ranks a product from a small business `r pander(non_log_intervals["small_businessTRUE", "2.5 %"], big.mark = 3)`% to `r pander(non_log_intervals["small_businessTRUE", "97.5 %"], big.mark = 3)`% further from the top. Perhaps Amazon assumes that products from small businesses have a more variable quality. However, this might disadvantage small businesses.
- When the number of ratings increases by 100, Amazon ranks a product `r pander(negative_hundred_intervals["number_of_ratings", "2.5 %"], big.mark = 3)`% to `r pander(negative_hundred_intervals["number_of_ratings", "97.5 %"], big.mark = 3)`% closer to the top. Perhaps Amazon assumes that products with many ratings are more popular.

Here is a histogram of the equivalent relevance boosts of sponsored listings, highlighting top 10 sponsored listings.
Again, I am defining the **equivalent relevance boost** as the increase in relevance necessary to achieve the same boost as sponsorship.
Again, Top 10 sponsored listings are sponsored listings that displace one of the top 10 organic results.

```{r, echo = FALSE, fig.cap="The relevance boosts of sponsored listings, highlighting top 10 sponsored listings", fig.id = "boosts"}
# Basic histogram
ggplot(
    bind_rows(
        complete_duplicates_data %>%
        mutate(Group = "All"),
        complete_duplicates_data %>%
        filter(displaced_organic_rank <= 10) %>%
        mutate(Group = "Top 10")
    )
) +
    aes(x = relevance_boost, fill = Group) +
    geom_histogram(binwidth = 2, position = "dodge") +
    xlab("Equivalent relevance boost in standard deviations") +
    ylab("Count")
```

Three observations:

- This size of the shift from sponsorship is often quite large. In some cases, to get the an equivalent boost as sponsorship, the relevance of the product to the query would have to increase by as much as 10 standard deviations.
- In general, the shift from sponsorship is nearly as likely to be up or down.
- However, if one only considers sponsored listings which displace one of the top 10 organic listings, the shift is nearly always upwards. This is almost certainly because the organic duplicate cannot appear much earlier than the sponsored listing, because the sponsored listing already appears high in the rankings.

# Discussion

Based on these results, I have come to the following conclusions.
If an Amazon seller sponsors their product (buys an ad), within each search in which the product appears, Amazon will sometimes create a duplicate sponsored listing for the product.
Amazon will then randomly shift the sponsored duplicate listing either up or down from the organic original.
This size of this shift is often large.
In some cases, to get the an equivalent boost as sponsorship, the relevance of the product to the query would have to increase by as much as 10 standard deviations.
In general, the shift is nearly as likely to be up or down.
However, if one only considers sponsored listings which displace one of the top 10 organic listings, the shift is nearly always upwards.
This is simply due to the fact that the organic duplicate cannot appear much earlier than the sponsored listing, because the sponsored listing already appears high in the rankings.
Thus, the average buyer, who only considers the top results, is likely to see sponsored products of much less lower quality than the displaced organic products.

# Appendix

For each product, I scraped the following variables:

- `amazon_brand`: Whether Amazon owns the product's brand
- `amazons_choice`: Whether Amazon labeled the product "Amazon's choice".
- `answered_questions`: The number of answered questions. Amazon reports "1000+" if there are more than 1000 answers, in which case, I used 1000.
- `best_seller_category`: Amazon chooses a handful of categories containing the product, and reports the product's best-seller rank within each (see `log_best_seller_rank` and `category` below). For this variable, I used the most general category listed with a best seller rank. Note that this might not be the same as `category` below.
- `category`: Amazon categorizes products into a nested tree, with more general categories containing more specific categories. I used the most general category listed.
- `climate_friendly`: Whether Amazon labeled the product "Climate Pledge Friendly".
- `coupon_percent`: The coupon amount as a percentage of the price (or 0 if no coupon available).
- `department`: The department of the product. Departments are similar to categories, but there are fewer departments, and Amazon does not nest departments.
- `discount_percent` The discount amount as a percent of the (undiscounted) price (or 0 for no discount).
- `fakespot_rating`: Fakespot rates products based on the amount of fake reviews Fakespot predicts a product has. Fakespot gives five ratings, in order: `A`, `B`, `C`, `D`, and `F`. Fakespot only reports ratings for products with enough reviews to calculate a ranking.
- `free_returns`: Whether returning the product is free.
- `limited_stock`: Whether there is a limited stock of the product.
- `log_best_seller_rank`: The log of the product's sales rank within its `best_seller_category` (see above).
- `log_unit_price`: The log of the unit price, the price per unit of the product (or per purchase for products with no units).
- `new_seller`: Fakespot reports whether the seller only recently started selling.
- `number_of_ratings`: The number of ratings.
- `rush_shipping_available`: Whether rush shipping is available.
- `ships_from_amazon`: Whether Amazon ships the product.
- `small_business`: Whether the seller has a small business badge.
- `sold_by_amazon`: Whether Amazon decides the price of the product.
- `standard_shipping_cost`: The cost for standard shipping.
- `standard_expected_shipping_days`: The expected number of days required for standard shipping.
- `standard_shipping_range`: The number of days within which the product might arrive.
- `subscribe_coupon`: Whether a coupon is available only if you subscribe.
- `subscription_available`: Whether buyers can subscribe to purchase the product, for example, purchase a shipment each month.
- `time`: The time of day I downloaded the product page.
- `unit`: The units of the product, for example, ounces, or "purchase" for products with no units.
- `one_star_percent`: The percentage of one-star reviews.
- `two_star_percent`: The percentage of two-star reviews.
- `four_star_percent`: The percentage of four-star reviews.
- `five_star_percent`: The percentage of five-star reviews.

For reference, here are the top 10 queries:

```{r, echo = FALSE, tab.cap = "Top 10 queries", tab.id = "queries"}
queries %>%
    rename(`Query` = query) %>%
    slice(1:10)
```

Here are the top 10 complete queries (no more than 20 pages of results):

```{r, echo = FALSE, tab.cap = "Top 10 queries with all results visible", tab.id = "complete_queries"}
sponsored_duplicates_data %>%
    select(query) %>%
    distinct %>%
    slice(1:10) %>%
    rename(Query = query)
```

The Poisson distribution is modeled on count data.
I used a Poisson distribution because the ranks are positive integers.
Although the ranks are not counts, empirically, the distribution fits fairly well.
You can see the fit of this distribution with the QQ plot below.
Because most of the residuals lie along the diagonal line, the distribution fits the data fairly well.

```{r, echo = FALSE, warning = FALSE, fig.cap = "QQ plot of Poisson residuals", fig.id = "qqplot"}
plot(main_model, which = 2)
```

I considered using random effects for the categorical variables, but when I combined the GLM with random effects, the estimates did not converge.
I also considered using a non-parametric "exploded logit" to predict the rankings, but again, the estimates did not converge.

Here is the regression output:

```{r, echo = FALSE, tab.cap = "Deviance residuals summary", tab.id = "deviance_residuals"}
summary(main_summary$deviance.resid) %>%
    as.list %>%
    as.data.frame(
        check.names = FALSE,
        row.names = rownames(.)
    )
```

```{r, echo = FALSE, tab.cap = "Selected coefficients", tab.id = "coefficients"}
main_summary$coefficients[c("log_best_seller_rank", "scaled_relevance_score"),] %>%
    as.data.frame(
        check.names = FALSE,
        row.names = rownames(.)
    ) %>%
    mutate(Coefficient = rownames(.)) %>%
    select(
        Coefficient,
        `Std. Error`,
        `z value`,
        `p values` = `Pr(>|z|)`
    )
```

```{r, echo = FALSE}
list(
    `Null deviance` = main_summary$null.deviance,
    `Null degrees of freedom` = main_summary$df.null,
    `Residual deviance` = main_summary$deviance,
    `Residual degrees of freedom` = main_summary$df.residual,
    AIC = main_summary$aic,
    `Iterations` = main_summary$iter
) %>%
    pander(big.mark = 3)
```

There is mixed evidence of Amazon self-preferencing.
Here are the coefficient interpretations for variables related to self-preferencing.
All else being equal, with 95% confidence,

- Amazon ranks an Amazon brand product `r pander(non_log_intervals["amazon_brandTRUE", "2.5 %"], big.mark = 3)`% to `r pander(non_log_intervals["amazon_brandTRUE", "97.5 %"], big.mark = 3)`% further from the top.
- Amazon ranks a product Amazon decides the price of ("sold by" Amazon) `r pander(negative_non_log_intervals["sold_by_amazonTRUE", "2.5 %"], big.mark = 3)`% to `r pander(negative_non_log_intervals["sold_by_amazonTRUE", "97.5 %"], big.mark = 3)`% closer to the top.
- Amazon ranks a product shipped by Amazon `r pander(negative_non_log_intervals["ships_from_amazonTRUE", "2.5 %"], big.mark = 3)`% to `r pander(negative_non_log_intervals["ships_from_amazonTRUE", "97.5 %"], big.mark = 3)`% closer to the top.

It is not clear why Amazon would self-preference products sold by or shipped by Amazon, but not Amazon brand products.

To test the robustness of the relevance coefficient, I conducted four additional regressions, and recalculated the relevance coefficient.

- In one regression, I excluded all variables not statistically significant at the $\alpha$=1% level
- In one regression, I excluded the Fakespot variable, which contained missing data. This increased the amount of data I could use (because fewer rows contained missing values).
- In one regression, I predicted log rank with a standard (non-Poisson) regression.
- In one regression, I only included the two most important variables, the log best-seller rank and the relevance scores, and the query fixed effects.

Here are the resulting raw estimates of the relevance coefficient from the robustness tests:

```{r, echo = FALSE}
raw_robustness_coefficients %>%
    as.list %>%
    pander(big.mark = 3)
```

For these estimates, the signs match, and the values are within the same order of magnitude.
Overall, this suggests that the relevance coefficient estimate is fairly robust.
